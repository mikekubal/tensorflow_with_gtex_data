{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set local path for GTEX data and supporting txt files\n",
    "ROOT_PATH = \"C:\\\\Users\\\\micha\\\\Downloads\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#total number of transcripts is about 56K\n",
    "#total number of samples is about 15K\n",
    "#experiment with various subsets\n",
    "num_of_genes = 500\n",
    "num_of_samples = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#order of samples in this file defines sample id\n",
    "sample_columns = os.path.join(ROOT_PATH, \"Sample_Columns.txt\")\n",
    "sample_file = open(sample_columns,'r')\n",
    "sample_names = []\n",
    "for line in sample_file:\n",
    "    parts = line.split('\\t')\n",
    "    for part in parts:\n",
    "        sample_name = part.rstrip()\n",
    "        sample_names.append(sample_name)\n",
    "\n",
    "#remove 'Name' and 'Description' from sample_names\n",
    "del(sample_names[0])\n",
    "del(sample_names[0])\n",
    "\n",
    "#create sample name->local id dictionary for later use\n",
    "sample_to_id = {}\n",
    "sample_id = 1\n",
    "\n",
    "for name in sample_names:\n",
    "    sample_to_id[name] = sample_id\n",
    "    sample_id = sample_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#keep track sample id to tissue id in dictionary\n",
    "sample_to_tissue = {}\n",
    "tissue_to_id = {}\n",
    "tissues = []\n",
    "sample_tissue_path = os.path.join(ROOT_PATH, \"Sample_to_Tissue.txt\")\n",
    "sample_tissue_file = open(sample_tissue_path,'r')\n",
    "#local tissue id starting with 1 is based on order listed in Sample_to_Tissue.txt\n",
    "tissue_id = 1\n",
    "for line in sample_tissue_file:\n",
    "    parts = line.split('\\t')\n",
    "    sample_name = parts[0].rstrip()\n",
    "    if (sample_name in sample_to_id.keys()):\n",
    "        sample_id = sample_to_id[sample_name]\n",
    "        tissue = parts[1].rstrip()\n",
    "        tissue = tissue.replace(',', ' ')\n",
    "        if (tissue not in tissue_to_id.keys()):\n",
    "            tissue_to_id[tissue] = tissue_id\n",
    "            tissues.append(tissue)\n",
    "            tissue_id = tissue_id + 1\n",
    "        sample_to_tissue[sample_id] = tissue_to_id[tissue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for the load_csv_with_header method used below\n",
    "# the header in the training and test files needs to be a comma-separated\n",
    "#list of number of entries, number of features, followed by list of all possible classes/categories\n",
    "header_string = str(num_of_samples) + \",\" + str(num_of_genes) +\",\" + \",\".join(tissues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#number of classes used as argument to classifier below\n",
    "num_of_classes = len(tissues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpm_data = os.path.join(ROOT_PATH, \"GTEx_Analysis_2016-01-15_v7_RNASeQCv1.1.8_gene_tpm.gct\")\n",
    "#skip the first 2 rows\n",
    "#in the reader the columns are the samples, rows are the genes\n",
    "#the matrix will be transposed in the csv file created below\n",
    "reader = pd.read_table(tpm_data, sep = \"\\t\",skiprows = [0,1], nrows=num_of_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#update the version of the csv file to experiment with different training and test sets\n",
    "TRAINING = \"training7.csv\"\n",
    "TEST = \"test7.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#need to reformat data for tf's load_csv_with_header\n",
    "#f is the file handle for the training data set\n",
    "f = open(TRAINING, \"w\")\n",
    "f.write(header_string + \"\\n\")\n",
    "#tpm values for sample1 begins in column 3 of the reader so the column index, \"i\" is initially set to 2 \n",
    "i = 2\n",
    "sample_id = 1\n",
    "while i < 2 + num_of_samples:\n",
    "    #temp list of tpm values for a given sample\n",
    "    list = []\n",
    "    tissue_id = sample_to_tissue[sample_id] \n",
    "    #each column contains expression values for each gene in a sample\n",
    "    for index, column in reader.iterrows() :\n",
    "        list.append(str(round(column[i],0)))\n",
    "    #need to add tissue id as the class/target value for the NN\n",
    "    #append tissue_id to end of line\n",
    "    list.append(str(tissue_id))\n",
    "    line = \",\".join(list) + \"\\n\"\n",
    "    f.write(line)\n",
    "    #use odd-numbered samples for the training set\n",
    "    #even-numbered samples will be used in the test set\n",
    "    sample_id =  sample_id + 2\n",
    "    #increment i by 2 to get corresponding tpm values for each odd-numbered sample\n",
    "    i = i + 2\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test data set will use even-numbered samples from the reader\n",
    "f_test = open(TEST, \"w\")\n",
    "f_test.write(header_string + \"\\n\")\n",
    "#column index for sample2 is 3\n",
    "i = 3\n",
    "sample_id = 2\n",
    "while i < 2 + num_of_samples:\n",
    "    list = []\n",
    "    tissue_id = sample_to_tissue[sample_id] \n",
    "    #each column contains expression values for each gene in a sample\n",
    "    for index, column in reader.iterrows() :\n",
    "        list.append(str(round(column[i],0)))\n",
    "    #append tissue_id to end of line, it will be used as the class/target used by NN classifier\n",
    "    list.append(str(tissue_id))\n",
    "    line = \",\".join(list) + \"\\n\"\n",
    "    f_test.write(line)\n",
    "    sample_id = sample_id + 2\n",
    "    i = i + 2\n",
    "f_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=TRAINING,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)\n",
    "test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=TEST,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify that all features have real-value data\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[num_of_genes])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\micha\\AppData\\Local\\Temp\\tmp_srzdq6m\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\micha\\\\AppData\\\\Local\\\\Temp\\\\tmp_srzdq6m', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                          hidden_units=[10, 20, 10],\n",
    "                                          n_classes=num_of_classes + 1,\n",
    "                                          model_dir=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the training inputs\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": np.array(training_set.data)},\n",
    "      y=np.array(training_set.target),\n",
    "      num_epochs=None,\n",
    "      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\micha\\AppData\\Local\\Temp\\tmp_srzdq6m\\model.ckpt.\n",
      "INFO:tensorflow:loss = 7489.19, step = 1\n",
      "INFO:tensorflow:global_step/sec: 457.26\n",
      "INFO:tensorflow:loss = 350.742, step = 101 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.279\n",
      "INFO:tensorflow:loss = 333.916, step = 201 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.276\n",
      "INFO:tensorflow:loss = 277.266, step = 301 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.237\n",
      "INFO:tensorflow:loss = 289.969, step = 401 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.271\n",
      "INFO:tensorflow:loss = 263.577, step = 501 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.276\n",
      "INFO:tensorflow:loss = 222.223, step = 601 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.26\n",
      "INFO:tensorflow:loss = 289.181, step = 701 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.227\n",
      "INFO:tensorflow:loss = 211.298, step = 801 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.325\n",
      "INFO:tensorflow:loss = 245.253, step = 901 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.255\n",
      "INFO:tensorflow:loss = 239.381, step = 1001 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.256\n",
      "INFO:tensorflow:loss = 202.768, step = 1101 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.249\n",
      "INFO:tensorflow:loss = 272.139, step = 1201 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.26\n",
      "INFO:tensorflow:loss = 192.111, step = 1301 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 534.935\n",
      "INFO:tensorflow:loss = 252.021, step = 1401 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.276\n",
      "INFO:tensorflow:loss = 214.826, step = 1501 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 581.715\n",
      "INFO:tensorflow:loss = 180.154, step = 1601 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.313\n",
      "INFO:tensorflow:loss = 286.647, step = 1701 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.277\n",
      "INFO:tensorflow:loss = 205.023, step = 1801 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.256\n",
      "INFO:tensorflow:loss = 239.487, step = 1901 (0.188 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into C:\\Users\\micha\\AppData\\Local\\Temp\\tmp_srzdq6m\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 277.16.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x1ead2aa29b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model.\n",
    "classifier.train(input_fn=train_input_fn, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": np.array(test_set.data)},\n",
    "      y=np.array(test_set.target),\n",
    "      num_epochs=1,\n",
    "      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-10-25-05:00:08\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\micha\\AppData\\Local\\Temp\\tmp_srzdq6m\\model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-25-05:00:08\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.5, average_loss = 1.72207, global_step = 2000, loss = 215.259\n",
      "\n",
      "Test Accuracy: 0.500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "\n",
    "print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load in median tpm values for each tissue type\n",
    "#test calssifier against median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classify one new tissue sample.\n",
    "\n",
    "new_samples = np.array(\n",
    "#test example with 100 genes - need to update if number of included genes change\n",
    "    [[0.0,7.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.0,322.0,30.0,9.0,7.0,2306.0,9.0,0.0,0.0,0.0,3.0,0.0,0.0,0.0,0.0,2.0,1.0,1.0,0.0,0.0,1.0,6.0,8.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,62.0,12.0,1.0,0.0,8.0,46.0,0.0,0.0,82.0,18.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,2.0,235.0,16.0,1.0,0.0,24.0,3.0,2.0,29.0,9.0,61.0,28.0,17.0,0.0,62.0]], dtype=np.float32)\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": new_samples},\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "predictions = classifier.predict(input_fn=predict_input_fn)\n",
    "#dir(predictions)\n",
    "d = next(predictions)\n",
    "print(type(d))\n",
    "print(d.keys())\n",
    "probs= d['probabilities']\n",
    "max_p_index = np.argmax(probs)\n",
    "print(max_p_index)\n",
    "print(tissues[max_p_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
